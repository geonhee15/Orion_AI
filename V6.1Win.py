import sys
import subprocess
import os
import datetime
import unicodedata
import base64
import cv2
import threading
import requests
import tempfile
import time
import mediapipe as mp
import pyautogui
import pygame
from anthropic import Anthropic
from tavily import TavilyClient
from pynput import keyboard
from dotenv import load_dotenv
from jamo import jamo_to_hcj
from google import genai
from PyQt6.QtWidgets import QApplication, QMainWindow, QLabel, QVBoxLayout, QPushButton, QWidget, QFrame
from PyQt6.QtCore import QTimer, Qt, pyqtSignal, QObject
from PyQt6.QtGui import QImage, QPixmap
from PIL import Image, ImageGrab
from winotify import Notification, audio

# Windows ì°½ í™œì„±í™”ìš©
try:
    import win32gui
    import win32con
    WIN32_AVAILABLE = True
except ImportError:
    WIN32_AVAILABLE = False
    print("âš ï¸ pywin32 ë¯¸ì„¤ì¹˜ - ì°½ í™œì„±í™” ê¸°ëŠ¥ ì œí•œë¨")

# 1. í™˜ê²½ ì„¤ì • ë° API ë¡œë“œ
load_dotenv()
anthropic_client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
tavily = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))
gemini_client = genai.Client(api_key=os.getenv("GOOGLE_API_KEY"))
ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY")

START_TRIGGER = "123enter"
EXIT_TRIGGER = "123exit"
SCREEN_TRIGGER = "screenmode"
CAMERA_TRIGGER = "cameramode"
GESTURE_TRIGGER = "gesturemode"
PLAYSONG_TRIGGER = "playsong"
STOPSONG_TRIGGER = "stopsong"
IGNORE_TRIGGER = "ignore"
IGNOREX_TRIGGER = "ignorex"
AI_NAME = "Orion"
PROFILE_FILE = "user_profile.txt"
TEMP_IMAGE = "temp_capture.png"
CLAUDE_MODEL = "claude-sonnet-4-5-20250929"
MUSIC_FOLDER = "Music"

# ElevenLabs ì„¤ì •
ELEVENLABS_VOICE_ID = "QYrOVogqhHWUzdZFXf0E"
ELEVENLABS_API_URL = f"https://api.elevenlabs.io/v1/text-to-speech/{ELEVENLABS_VOICE_ID}"

# ì“°ë ˆë“œ ê°„ UI í†µì‹ ì„ ìœ„í•œ ì‹ í˜¸ ê´€ë¦¬ì
class SignalManager(QObject):
    show_camera = pyqtSignal()
    close_camera = pyqtSignal()
    show_debug = pyqtSignal()
    hide_debug = pyqtSignal()
    update_debug_frame = pyqtSignal(object)

# --- [ìŒì•… í”Œë ˆì´ì–´ í´ë˜ìŠ¤ - pygame ê¸°ë°˜ ì‹¤ì‹œê°„ ë³¼ë¥¨ ì¡°ì ˆ] ---
class MusicPlayer:
    def __init__(self):
        pygame.mixer.init()
        self.is_playing = False
        self.current_song = None
        self.current_filepath = None
        self.normal_volume = 0.2
        self.ducked_volume = 0.05
    
    def duck(self):
        """TTS ì‹œì‘ ì‹œ ìŒì•… ë³¼ë¥¨ ë‚®ì¶”ê¸°"""
        if self.is_playing:
            pygame.mixer.music.set_volume(self.ducked_volume)
            print(f"ğŸ”‰ ìŒì•… ë³¼ë¥¨ ë‚®ì¶¤: {self.normal_volume} â†’ {self.ducked_volume}")
    
    def unduck(self):
        """TTS ëë‚˜ë©´ ìŒì•… ë³¼ë¥¨ ë³µêµ¬"""
        if self.is_playing:
            pygame.mixer.music.set_volume(self.normal_volume)
            print(f"ğŸ”Š ìŒì•… ë³¼ë¥¨ ë³µêµ¬: {self.ducked_volume} â†’ {self.normal_volume}")
    
    def play(self, song_name):
        """ë…¸ë˜ ì¬ìƒ (ë¬´í•œ ë°˜ë³µ)"""
        self.stop()
        
        filename = song_name.strip().replace(" ", "_")
        if not filename.lower().endswith(".mp3"):
            filename += ".mp3"
        
        filepath = os.path.join(MUSIC_FOLDER, filename)
        
        if not os.path.exists(filepath):
            if os.path.exists(MUSIC_FOLDER):
                for f in os.listdir(MUSIC_FOLDER):
                    if f.lower() == filename.lower():
                        filepath = os.path.join(MUSIC_FOLDER, f)
                        break
                else:
                    print(f"âŒ ìŒì•… íŒŒì¼ ì—†ìŒ: {filepath}")
                    return False
            else:
                print(f"âŒ Music í´ë” ì—†ìŒ!")
                return False
        
        try:
            pygame.mixer.music.load(filepath)
            pygame.mixer.music.set_volume(self.normal_volume)
            pygame.mixer.music.play(loops=-1)
            self.is_playing = True
            self.current_song = song_name
            self.current_filepath = filepath
            print(f"ğŸµ ì¬ìƒ ì‹œì‘: {filename}")
            return True
        except Exception as e:
            print(f"ì¬ìƒ ì—ëŸ¬: {e}")
            return False
    
    def stop(self):
        """ì¬ìƒ ì¤‘ì§€"""
        if self.is_playing:
            pygame.mixer.music.stop()
        self.is_playing = False
        self.current_song = None
        self.current_filepath = None
        print("ğŸ›‘ ìŒì•… ì¤‘ì§€ë¨")

# --- [TTS ì „ìš© ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´ (pygame Sound ê°ì²´)] ---
class TTSPlayer:
    """ìŒì•…ê³¼ ë³„ë„ë¡œ TTSë¥¼ ì¬ìƒí•˜ê¸° ìœ„í•œ í”Œë ˆì´ì–´"""
    def __init__(self):
        # pygame.mixerëŠ” MusicPlayerì—ì„œ ì´ë¯¸ ì´ˆê¸°í™”ë¨
        pass
    
    def play_file(self, filepath):
        """mp3 íŒŒì¼ì„ Sound ê°ì²´ë¡œ ì¬ìƒ (ìŒì•…ê³¼ ë™ì‹œ ì¬ìƒ ê°€ëŠ¥)"""
        try:
            # pygame.mixer.SoundëŠ” wavë§Œ ì§€ì›í•˜ë¯€ë¡œ
            # ì„ì‹œë¡œ subprocessë¡œ Windows Media Player ì‚¬ìš©í•˜ê±°ë‚˜
            # ffmpegë¡œ ë³€í™˜ í›„ ì¬ìƒ
            
            # ë°©ë²• 1: Windows ê¸°ë³¸ í”Œë ˆì´ì–´ ì‚¬ìš© (ê°€ì¥ ê°„ë‹¨)
            import winsound
            # winsoundëŠ” wavë§Œ ì§€ì›í•˜ë¯€ë¡œ ë‹¤ë¥¸ ë°©ë²• ì‚¬ìš©
            
            # ë°©ë²• 2: playsound ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì¶”ì²œ)
            try:
                from playsound import playsound
                playsound(filepath, block=True)
            except ImportError:
                # ë°©ë²• 3: pygame mixerì˜ music ì±„ë„ ì„ì‹œ ì‚¬ìš©
                # (í˜„ì¬ ìŒì•… ì¼ì‹œì •ì§€ â†’ TTS ì¬ìƒ â†’ ìŒì•… ì¬ê°œ)
                current_pos = 0
                was_playing = pygame.mixer.music.get_busy()
                
                if was_playing:
                    current_pos = pygame.mixer.music.get_pos()
                    pygame.mixer.music.pause()
                
                # TTS ì¬ìƒ
                pygame.mixer.music.load(filepath)
                pygame.mixer.music.play()
                while pygame.mixer.music.get_busy():
                    time.sleep(0.1)
                
                # ì›ë˜ ìŒì•… ë³µì› (ë³µì¡í•˜ë¯€ë¡œ ìƒëµ, playsound ì„¤ì¹˜ ê¶Œì¥)
                
        except Exception as e:
            print(f"TTS ì¬ìƒ ì—ëŸ¬: {e}")

# --- [ê³µìœ  ì¹´ë©”ë¼ ë§¤ë‹ˆì €] ---
class SharedCamera:
    _instance = None
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized: return
        self._initialized = True
        self.cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)  # Windows: DirectShow ì‚¬ìš©
        self.lock = threading.Lock()
        self.last_frame = None
    
    def read(self):
        with self.lock:
            ret, frame = self.cap.read()
            if ret: self.last_frame = frame.copy()
            return ret, self.last_frame

# --- [ì œìŠ¤ì²˜ ì¸ì‹ ì»¨íŠ¸ë¡¤ëŸ¬] ---
class GestureController:
    def __init__(self, shared_camera, signals):
        self.is_running = False
        self.shared_camera = shared_camera
        self.signals = signals
        self.mp_hands = mp.solutions.hands
        self.mp_draw = mp.solutions.drawing_utils
        self.hands = self.mp_hands.Hands(
            static_image_mode=False, max_num_hands=1,
            model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5
        )
        self.prev_state = None
        self.stable_count = 0
        self.last_time = 0

    def start(self):
        if self.is_running: return
        self.is_running = True
        threading.Thread(target=self._run, daemon=True).start()

    def stop(self):
        self.is_running = False

    def _run(self):
        while self.is_running:
            ret, frame = self.shared_camera.read()
            if not ret or frame is None:
                time.sleep(0.01)
                continue
            
            frame = cv2.flip(frame, 1)
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = self.hands.process(rgb)
            
            debug_frame = frame.copy()
            if results.multi_hand_landmarks:
                for hl in results.multi_hand_landmarks:
                    self.mp_draw.draw_landmarks(debug_frame, hl, self.mp_hands.HAND_CONNECTIONS)
                    self._process_gesture(hl)
            
            self.signals.update_debug_frame.emit(debug_frame)
            time.sleep(0.02)

    def _process_gesture(self, hl):
        tip, pip = hl.landmark[8], hl.landmark[6]
        is_ext = pip.y - tip.y > 0.06
        curr = "UP" if is_ext else "DOWN"
        
        if curr == self.prev_state: self.stable_count += 1
        else:
            self.stable_count = 1
            self.prev_state = curr
            return

        now = time.time()
        if now - self.last_time > 0.6 and self.stable_count >= 3:
            if curr == "UP": pyautogui.scroll(12)
            else: pyautogui.scroll(-12)
            self.last_time = now

# --- [PyQt ê¸°ë°˜ ë””ë²„ê·¸ ìœˆë„ìš° (ì†Œí˜•, ìš°í•˜ë‹¨, í•­ìƒ ìµœìƒë‹¨, í´ë¦­ íˆ¬ê³¼)] ---
class DebugWindow(QMainWindow):
    def __init__(self, signals):
        super().__init__()
        self.setWindowFlags(
            Qt.WindowType.FramelessWindowHint |
            Qt.WindowType.WindowStaysOnTopHint |
            Qt.WindowType.Tool  # Windowsì—ì„œëŠ” Tool í”Œë˜ê·¸ ì‚¬ìš©
        )
        self.setAttribute(Qt.WidgetAttribute.WA_TranslucentBackground)
        
        # ì†Œí˜• ì‚¬ì´ì¦ˆ
        width, height = 120, 90
        screen = QApplication.primaryScreen().geometry()
        self.setGeometry(screen.width() - width - 10, screen.height() - height - 80, width, height)
        
        self.container = QFrame(self)
        self.container.setStyleSheet("background-color: rgba(0, 0, 0, 150); border-radius: 8px;")
        self.container.setFixedSize(width, height)
        
        self.label = QLabel(self.container)
        self.label.setFixedSize(width - 10, height - 10)
        self.label.move(5, 5)
        self.setCentralWidget(self.container)
        signals.update_debug_frame.connect(self.set_image)
        
        # Windows í´ë¦­ íˆ¬ê³¼ ì„¤ì •
        self._set_click_through()

    def _set_click_through(self):
        """Windowsì—ì„œ í´ë¦­ íˆ¬ê³¼ ì„¤ì •"""
        if WIN32_AVAILABLE:
            try:
                hwnd = int(self.winId())
                # WS_EX_TRANSPARENT | WS_EX_LAYERED
                extended_style = win32gui.GetWindowLong(hwnd, win32con.GWL_EXSTYLE)
                win32gui.SetWindowLong(
                    hwnd, 
                    win32con.GWL_EXSTYLE, 
                    extended_style | win32con.WS_EX_TRANSPARENT | win32con.WS_EX_LAYERED
                )
            except Exception as e:
                print(f"í´ë¦­ íˆ¬ê³¼ ì„¤ì • ì‹¤íŒ¨: {e}")

    def set_image(self, cv_img):
        rgb = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)
        h, w, ch = rgb.shape
        img = QImage(rgb.data, w, h, ch*w, QImage.Format.Format_RGB888)
        self.label.setPixmap(QPixmap.fromImage(img).scaled(110, 80, Qt.AspectRatioMode.KeepAspectRatio))

# --- [ë¦¬í€´ë“œ ê¸€ë˜ìŠ¤ ìŠ¤íƒ€ì¼ ì¹´ë©”ë¼ ìœ„ì ¯] ---
class CameraWindow(QMainWindow):
    def __init__(self, shared_camera, capture_callback):
        super().__init__()
        self.shared_camera = shared_camera
        self.capture_callback = capture_callback
        
        self.setWindowFlags(
            Qt.WindowType.FramelessWindowHint | 
            Qt.WindowType.WindowStaysOnTopHint | 
            Qt.WindowType.Tool
        )
        self.setAttribute(Qt.WidgetAttribute.WA_TranslucentBackground)
        
        screen = QApplication.primaryScreen().geometry()
        width, height = 115, 145
        self.setGeometry(100, 100, width, height)
        print(f"ğŸ“ í™”ë©´ í¬ê¸°: {screen.width()}x{screen.height()}")
        print(f"ğŸ“ ì°½ ì„¤ì • ìœ„ì¹˜: (100, 100, {width}, {height})")

        self.container = QFrame(self)
        self.container.setStyleSheet("""
            QFrame {
                background-color: rgba(15, 15, 22, 180);
                border: 1px solid rgba(255, 255, 255, 30);
                border-radius: 12px;
            }
        """)
        self.container.setFixedSize(width, height)

        self.layout = QVBoxLayout(self.container)
        self.layout.setContentsMargins(6, 6, 6, 6)
        self.layout.setSpacing(5)

        self.image_label = QLabel()
        self.image_label.setStyleSheet("border-radius: 8px; background: #000;")
        self.image_label.setFixedSize(103, 80)
        self.layout.addWidget(self.image_label, alignment=Qt.AlignmentFlag.AlignCenter)

        self.btn_identify = QPushButton("Identify")
        self.btn_identify.setStyleSheet("""
            QPushButton {
                background-color: rgba(60, 130, 250, 170);
                color: white;
                border-radius: 6px;
                font-size: 10px;
                font-weight: bold;
                padding: 4px;
            }
            QPushButton:hover { background-color: rgba(80, 150, 255, 220); }
        """)
        self.btn_identify.clicked.connect(self.take_photo)
        self.layout.addWidget(self.btn_identify)

        self.setCentralWidget(self.container)
        
        if not self.shared_camera.cap.isOpened():
            print("âŒ ì¹´ë©”ë¼ ì—´ê¸° ì‹¤íŒ¨! ê¶Œí•œ í™•ì¸ í•„ìš”")
        else:
            print("âœ… ì¹´ë©”ë¼ ì—°ê²° ì„±ê³µ")
        
        self.timer = QTimer()
        self.timer.timeout.connect(self.update_frame)
        self.timer.start(30)

    def show(self):
        print("âœ… CameraWindow.show() í˜¸ì¶œë¨")
        super().show()
        self.raise_()
        self.activateWindow()
        print(f"ğŸ“ ì‹¤ì œ ì°½ ìœ„ì¹˜: {self.geometry().x()}, {self.geometry().y()}")
        print(f"ğŸ‘ï¸ ì°½ visible ìƒíƒœ: {self.isVisible()}")

    def update_frame(self):
        ret, frame = self.shared_camera.read()
        if ret and frame is not None:
            frame = cv2.flip(frame, 1)
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            h, w, ch = frame.shape
            q_img = QImage(frame.data, w, h, ch * w, QImage.Format.Format_RGB888)
            pixmap = QPixmap.fromImage(q_img).scaled(103, 80, Qt.AspectRatioMode.KeepAspectRatioByExpanding, Qt.TransformationMode.SmoothTransformation)
            self.image_label.setPixmap(pixmap)

    def take_photo(self):
        ret, frame = self.shared_camera.read()
        if ret:
            cv2.imwrite(TEMP_IMAGE, frame)
            threading.Thread(target=self.capture_callback, daemon=True).start()

    def close_cam(self):
        print("ğŸ”´ CameraWindow.close_cam() í˜¸ì¶œë¨")
        self.hide()

# --- [ë©”ì¸ ë´‡ í´ë˜ìŠ¤: ì˜¤ë¦¬ì˜¨ V6.1 Windows] ---
class OrionBot:
    def __init__(self, signal_manager, shared_camera):
        self.is_active = False
        self.screen_mode_waiting = False 
        self.ignore_mode = False
        self.full_input = ""
        self.short_term_memory = []
        self.signals = signal_manager
        self.shared_camera = shared_camera
        self.gesture_ctrl = GestureController(shared_camera, signal_manager)
        self.gesture_active = False
        self.music_player = MusicPlayer()
        self.load_personal_profile()

    def load_personal_profile(self):
        extra_info = ""
        if os.path.exists(PROFILE_FILE):
            with open(PROFILE_FILE, "r", encoding="utf-8") as f:
                extra_info = f.read()
        
        self.system_prompt = (
            f"ë‹¹ì‹ ì€ ê±´í¬ì˜ ë² í”„ì´ì ì „ìš© AI ë¹„ì„œ '{AI_NAME}'ì´ì•¼! ã…‹ã…‹\n"
            f"[ê±´í¬ ì •ë³´]\n{extra_info}\n"
            "í•µì‹¬ ì§€ì¹¨:\n"
            "1. ë¬´ì¡°ê±´ 'ë°˜ë§'ë¡œ ì¹œêµ¬ì²˜ëŸ¼ ë°ê²Œ ë§í•´ì¤˜!\n"
            "2. ë‹µë³€ì€ ì•Œë¦¼ì°½ìš©ì´ë‹ˆê¹Œ ë¬´ì¡°ê±´ 'í•œ ë¬¸ì¥'ìœ¼ë¡œ ì•„ì£¼ ì§§ê³  í•µì‹¬ë§Œ ë§í•´.\n"
            "3. ì´ë¯¸ì§€ ë¶„ì„ ì‹œì—ëŠ” ì•„ì£¼ êµ¬ì²´ì ì´ê³  ì¬ì¹˜ ìˆê²Œ ì„¤ëª…í•´ì¤˜.\n"
            "4. ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê¸°ì–µí•´ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€ì¤˜."
        )

    def fix_hangul(self, text):
        try:
            combined = jamo_to_hcj(text)
            return unicodedata.normalize('NFC', combined)
        except:
            return unicodedata.normalize('NFC', text)

    def activate_python_app(self):
        """ìµœì†Œí™”ëœ íŒŒì´ì¬ ì•±ì„ í™”ë©´ ë§¨ ì•ìœ¼ë¡œ ê°•ì œ í™œì„±í™” (Windows)"""
        if not WIN32_AVAILABLE:
            print("âš ï¸ pywin32 ì—†ìŒ - ì°½ í™œì„±í™” ìŠ¤í‚µ")
            return
            
        try:
            def callback(hwnd, hwnds):
                title = win32gui.GetWindowText(hwnd).lower()
                if "python" in title or "orion" in title:
                    hwnds.append(hwnd)
                return True
            
            hwnds = []
            win32gui.EnumWindows(callback, hwnds)
            
            for hwnd in hwnds:
                try:
                    win32gui.ShowWindow(hwnd, win32con.SW_RESTORE)
                    win32gui.SetForegroundWindow(hwnd)
                except:
                    pass
            print("ğŸ”„ Python ì•± í™œì„±í™” ì™„ë£Œ")
        except Exception as e:
            print(f"App Activation Error: {e}")

    def capture_screen(self):
        """Windows ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ (ì „ì²´ í™”ë©´)"""
        try:
            # PIL ImageGrabìœ¼ë¡œ ì „ì²´ í™”ë©´ ìº¡ì²˜
            screenshot = ImageGrab.grab()
            screenshot.save(TEMP_IMAGE)
            return os.path.exists(TEMP_IMAGE)
        except Exception as e:
            print(f"ìŠ¤í¬ë¦°ìƒ· ì—ëŸ¬: {e}")
            # ëŒ€ì•ˆ: pyautogui ì‚¬ìš©
            try:
                screenshot = pyautogui.screenshot()
                screenshot.save(TEMP_IMAGE)
                return os.path.exists(TEMP_IMAGE)
            except:
                return False

    def capture_screen_region(self):
        """Windows ì˜ì—­ ì„ íƒ ìŠ¤í¬ë¦°ìƒ· (Snipping Tool í˜¸ì¶œ)"""
        try:
            # Windows Snipping Tool ì‹¤í–‰
            subprocess.run(["snippingtool", "/clip"], shell=True)
            # í´ë¦½ë³´ë“œì—ì„œ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
            time.sleep(2)  # ì‚¬ìš©ìê°€ ì˜ì—­ ì„ íƒí•  ì‹œê°„
            img = ImageGrab.grabclipboard()
            if img:
                img.save(TEMP_IMAGE)
                return True
            return False
        except Exception as e:
            print(f"ì˜ì—­ ìº¡ì²˜ ì—ëŸ¬: {e}")
            # ì „ì²´ í™”ë©´ ìº¡ì²˜ë¡œ í´ë°±
            return self.capture_screen()

    def translate_to_english(self, korean_text):
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­"""
        try:
            response = anthropic_client.messages.create(
                model=CLAUDE_MODEL,
                max_tokens=200,
                messages=[{
                    "role": "user", 
                    "content": f"Translate this Korean text to natural English. The name 'ê±´í¬' should be written as 'Gun-hee'. Only output the translation, nothing else:\n\n{korean_text}"
                }]
            )
            result = response.content[0].text.strip()
            
            result = result.replace("Geonhee", "Gun-hee")
            result = result.replace("Gunhee", "Gun-hee")
            result = result.replace("Keonhee", "Gun-hee")
            result = result.replace("ê±´í¬", "Gun-hee")
            
            return result
        except Exception as e:
            print(f"Translation Error: {e}")
            return korean_text

    def speak_with_elevenlabs(self, text):
        """ElevenLabs TTSë¡œ ì˜ì–´ ìŒì„± ì¶œë ¥ (Windows) + ìŒì•… ë³¼ë¥¨ ìë™ ì¡°ì ˆ"""
        def _speak():
            try:
                self.music_player.duck()
                
                english_text = self.translate_to_english(text)
                print(f"[TTS] ë²ˆì—­ë¨: {english_text}")
                
                headers = {
                    "Accept": "audio/mpeg",
                    "Content-Type": "application/json",
                    "xi-api-key": ELEVENLABS_API_KEY
                }
                
                data = {
                    "text": english_text,
                    "model_id": "eleven_turbo_v2_5",
                    "voice_settings": {
                        "stability": 0.5,
                        "similarity_boost": 0.75,
                        "style": 0.3,
                        "use_speaker_boost": True
                    }
                }
                
                response = requests.post(ELEVENLABS_API_URL, json=data, headers=headers)
                
                if response.status_code == 200:
                    with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as f:
                        f.write(response.content)
                        temp_path = f.name
                    
                    # Windows TTS ì¬ìƒ ë°©ë²•ë“¤ (ìš°ì„ ìˆœìœ„)
                    played = False
                    
                    # ë°©ë²• 1: playsound (ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´)
                    try:
                        from playsound import playsound
                        playsound(temp_path, block=True)
                        played = True
                    except ImportError:
                        pass
                    
                    # ë°©ë²• 2: pygame mixer (ìŒì•… ì¼ì‹œ ì¤‘ì§€ í›„ ì¬ìƒ)
                    if not played:
                        try:
                            was_playing = self.music_player.is_playing
                            if was_playing:
                                pygame.mixer.music.pause()
                            
                            # TTSìš© Sound ê°ì²´ë¡œ ì¬ìƒ ì‹œë„
                            pygame.mixer.init()
                            tts_sound = pygame.mixer.Sound(temp_path)
                            tts_sound.play()
                            while pygame.mixer.get_busy():
                                time.sleep(0.1)
                            
                            if was_playing:
                                pygame.mixer.music.unpause()
                            played = True
                        except Exception as e:
                            print(f"pygame TTS ì—ëŸ¬: {e}")
                    
                    # ë°©ë²• 3: Windows Media Player via COM (ìµœí›„ì˜ ìˆ˜ë‹¨)
                    if not played:
                        try:
                            os.startfile(temp_path)
                            time.sleep(3)  # ì¬ìƒ ëŒ€ê¸°
                            played = True
                        except:
                            pass
                    
                    # ì„ì‹œ íŒŒì¼ ì‚­ì œ (ì•½ê°„ì˜ ë”œë ˆì´ í›„)
                    try:
                        time.sleep(0.5)
                        os.remove(temp_path)
                    except:
                        pass
                else:
                    print(f"[TTS Error] Status: {response.status_code}, {response.text}")
                    
            except Exception as e:
                print(f"[TTS Error] {e}")
            finally:
                self.music_player.unduck()
        
        threading.Thread(target=_speak, daemon=True).start()

    def get_vision_response(self, user_text, image_path):
        """ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ ë¶„ì„ (Claude)"""
        try:
            with open(image_path, "rb") as f:
                image_data = base64.b64encode(f.read()).decode("utf-8")

            response = anthropic_client.messages.create(
                model=CLAUDE_MODEL,
                max_tokens=300,
                system=self.system_prompt,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "image", "source": {"type": "base64", "media_type": "image/png", "data": image_data}},
                            {"type": "text", "text": user_text}
                        ]
                    }
                ]
            )
            if os.path.exists(image_path): os.remove(image_path)
            return response.content[0].text.strip()
        except Exception as e:
            return f"ì´ë¯¸ì§€ ë¶„ì„í•˜ë‹¤ê°€ ë ‰ ê±¸ë ¸ì–´ ã… ã… : {str(e)}"

    def get_gemini_vision(self):
        """ì‹¤ì‹œê°„ ì¹´ë©”ë¼ ë¶„ì„ (Gemini 2.0 Flash)"""
        try:
            img = Image.open(TEMP_IMAGE)

            response = gemini_client.models.generate_content(
                model="gemini-2.0-flash", 
                contents=[
                    self.system_prompt + "\nì´ ì´ë¯¸ì§€ë¥¼ ë³´ê³  ì¬ì¹˜ ìˆê²Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ë§í•´ì¤˜!",
                    img
                ]
            )
            
            answer = response.text.strip()
            self.notify(answer)
            self.speak_with_elevenlabs(answer)
            
            img.close()
            if os.path.exists(TEMP_IMAGE): 
                os.remove(TEMP_IMAGE)
                
        except Exception as e:
            print(f"Gemini Vision Error: {e}")
            self.notify("ëª¨ë¸ì„ ëª» ì°¾ê² ëŒ€! ì´ë¦„ì„ ë‹¤ì‹œ í™•ì¸í•´ë³¼ê²Œ.")

    def get_ai_response(self, user_text):
        """ëª¨ë“  ëŒ€í™”/ê²€ìƒ‰/ì‚¬ê³  ë¡œì§ + ì‹œê°„/ë‚ ì”¨/ë‰´ìŠ¤ ê°•í™”"""
        try:
            user_text = self.fix_hangul(user_text)
            
            now = datetime.datetime.now()
            time_info = f"[í˜„ì¬ ì‹œê°: {now.strftime('%Yë…„ %mì›” %dì¼ %A %Hì‹œ %Më¶„')}]"
            
            force_search_keywords = ["ë‚ ì”¨", "ë‰´ìŠ¤", "ì˜¤ëŠ˜", "ìµœê·¼", "í˜„ì¬", "ì§€ê¸ˆ", "ì‹¤ì‹œê°„", "weather", "news"]
            needs_force_search = any(kw in user_text.lower() for kw in force_search_keywords)
            
            context = ""
            
            if needs_force_search:
                search_prompt = anthropic_client.messages.create(
                    model=CLAUDE_MODEL,
                    max_tokens=50,
                    messages=[{"role": "user", "content": f"'{user_text}'ë¥¼ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ ì˜ì–´ ê²€ìƒ‰ì–´ í•˜ë‚˜ë§Œ ì¶œë ¥í•´. ì˜ˆ: 'Seoul weather today'"}]
                )
                query = search_prompt.content[0].text.strip()
                res = tavily.search(query=query, search_depth="advanced", max_results=3)
                context = "\n\n[ì‹¤ì‹œê°„ ì •ë³´]: " + "\n".join([r['content'] for r in res['results']])
            else:
                thought_res = anthropic_client.messages.create(
                    model=CLAUDE_MODEL,
                    max_tokens=100,
                    messages=[{"role": "user", "content": f"ì§ˆë¬¸: '{user_text}'\nê²€ìƒ‰ í•„ìš”ì‹œ 'SEARCH: [ì˜ì–´ê²€ìƒ‰ì–´]', ë¶ˆí•„ìš”ì‹œ 'NO'ë§Œ ëŒ€ë‹µ."}]
                )
                thought = thought_res.content[0].text.strip()
                
                if "SEARCH:" in thought.upper():
                    query = thought.split(":", 1)[1].strip()
                    res = tavily.search(query=query, search_depth="advanced", max_results=3)
                    context = "\n\n[ì‹¤ì‹œê°„ ì •ë³´]: " + "\n".join([r['content'] for r in res['results']])

            messages = [{"role": m["role"], "content": m["content"]} for m in self.short_term_memory]
            messages.append({"role": "user", "content": f"{time_info}\n{user_text} {context}"})

            response = anthropic_client.messages.create(
                model=CLAUDE_MODEL,
                max_tokens=300,
                system=self.system_prompt,
                messages=messages
            )
            answer = response.content[0].text.strip()
            
            self.short_term_memory.append({"role": "user", "content": user_text})
            self.short_term_memory.append({"role": "assistant", "content": answer})
            if len(self.short_term_memory) > 10: self.short_term_memory.pop(0)
            
            return answer
        except Exception as e:
            return f"ì—”ì§„ ê³¼ë¶€í•˜! ã… ã… : {str(e)}"

    def notify(self, msg):
        """Windows 11 ë„¤ì´í‹°ë¸Œ ì•Œë¦¼ (winotify)"""
        try:
            # ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ (Windows ì•Œë¦¼ì€ ê¸´ í…ìŠ¤íŠ¸ ì˜ë¦¼)
            if len(msg) > 200:
                msg = msg[:197] + "..."
            
            toast = Notification(
                app_id=AI_NAME,
                title=AI_NAME,
                msg=msg,
                duration="short"  # "short" ë˜ëŠ” "long"
            )
            
            # ì•Œë¦¼ìŒ ì„¤ì • (ì„ íƒì‚¬í•­)
            toast.set_audio(audio.Default, loop=False)
            
            # ì•Œë¦¼ í‘œì‹œ
            toast.show()
            
        except Exception as e:
            print(f"ì•Œë¦¼ ì—ëŸ¬: {e}")
            # í´ë°±: ì½˜ì†” ì¶œë ¥
            print(f"ğŸ“¢ [{AI_NAME}] {msg}")

    def on_press(self, key):
        try:
            if hasattr(key, 'char') and key.char:
                self.full_input += key.char
            elif key == keyboard.Key.enter:
                cmd = self.full_input.strip()
                cmd_lower = cmd.lower()
                print(f"ğŸ”¤ ì…ë ¥ëœ ëª…ë ¹: '{cmd}'")
                
                if not self.is_active:
                    if cmd.endswith(START_TRIGGER):
                        self.is_active = True
                        print("ğŸŸ¢ ì˜¤ë¦¬ì˜¨ í™œì„±í™”ë¨")
                        self.notify("ì˜¤ë¦¬ì˜¨ V6.1 Windows ì—°ê²° ì™„ë£Œ!")
                        self.speak_with_elevenlabs("ì˜¤ë¦¬ì˜¨ V6.1 ì—°ê²° ì™„ë£Œ!")
                elif self.is_active:
                    if self.ignore_mode:
                        if cmd_lower == IGNOREX_TRIGGER:
                            self.ignore_mode = False
                            print("ğŸ”Š Ignore ëª¨ë“œ í•´ì œë¨")
                            self.notify("ì±„íŒ… ê°ì§€ ì¬ê°œ!")
                            self.speak_with_elevenlabs("ë‹¤ì‹œ ë“¤ì„ê²Œ!")
                        self.full_input = ""
                        return
                    
                    if cmd_lower == IGNORE_TRIGGER:
                        self.ignore_mode = True
                        print("ğŸ”‡ Ignore ëª¨ë“œ ì§„ì…")
                        self.notify("ì±„íŒ… ê°ì§€ ì¼ì‹œì •ì§€! (ignorexë¡œ í•´ì œ)")
                        self.speak_with_elevenlabs("ì ê¹ ì‰´ê²Œ!")
                    
                    elif cmd.endswith(EXIT_TRIGGER):
                        self.is_active = False
                        self.ignore_mode = False
                        self.gesture_ctrl.stop()
                        self.music_player.stop()
                        self.signals.hide_debug.emit()
                        self.signals.close_camera.emit()
                        self.notify("í‡´ê·¼í•œë‹¤! ì´ë”° ë´!")
                        self.speak_with_elevenlabs("í‡´ê·¼í•œë‹¤! ì´ë”° ë´!")
                    
                    elif cmd_lower.startswith(PLAYSONG_TRIGGER):
                        song_name = cmd[len(PLAYSONG_TRIGGER):].strip()
                        if song_name:
                            if self.music_player.play(song_name):
                                self.notify(f"ğŸµ {song_name} ì¬ìƒ ì¤‘!")
                                self.speak_with_elevenlabs(f"{song_name} í‹€ì–´ì¤„ê²Œ!")
                            else:
                                self.notify(f"âŒ {song_name} íŒŒì¼ì„ ëª» ì°¾ê² ì–´!")
                                self.speak_with_elevenlabs(f"{song_name} íŒŒì¼ì´ ì—†ì–´!")
                        else:
                            self.notify("ë…¸ë˜ ì´ë¦„ì„ ì…ë ¥í•´ì¤˜!")
                            self.speak_with_elevenlabs("ë…¸ë˜ ì´ë¦„ ì•Œë ¤ì¤˜!")
                    
                    elif cmd_lower == STOPSONG_TRIGGER:
                        if self.music_player.current_song:
                            song = self.music_player.current_song
                            self.music_player.stop()
                            self.notify(f"ğŸ›‘ {song} ì¤‘ì§€!")
                            self.speak_with_elevenlabs("ìŒì•… ê»ì–´!")
                        else:
                            self.notify("ì¬ìƒ ì¤‘ì¸ ìŒì•…ì´ ì—†ì–´!")
                            self.speak_with_elevenlabs("ì§€ê¸ˆ ì¬ìƒ ì¤‘ì¸ ìŒì•… ì—†ì–´!")
                    
                    elif cmd_lower == GESTURE_TRIGGER:
                        self.gesture_active = not self.gesture_active
                        if self.gesture_active:
                            self.gesture_ctrl.start()
                            self.signals.show_debug.emit()
                            self.notify("ì œìŠ¤ì²˜ ëª¨ë“œ ON!")
                            self.speak_with_elevenlabs("ì œìŠ¤ì²˜ ëª¨ë“œ ON!")
                        else:
                            self.gesture_ctrl.stop()
                            self.signals.hide_debug.emit()
                            self.notify("ì œìŠ¤ì²˜ ëª¨ë“œ OFF!")
                            self.speak_with_elevenlabs("ì œìŠ¤ì²˜ ëª¨ë“œ OFF!")
                    
                    elif cmd_lower == CAMERA_TRIGGER:
                        print("ğŸ¯ ì¹´ë©”ë¼ íŠ¸ë¦¬ê±° ê°ì§€ë¨")
                        self.activate_python_app()
                        print("ğŸ“¡ show_camera ì‹œê·¸ë„ emit ì „")
                        self.signals.show_camera.emit()
                        print("ğŸ“¡ show_camera ì‹œê·¸ë„ emit í›„")
                        self.notify("ì¹´ë©”ë¼ ëª¨ë“œ ì¼ ë‹¤! ã…‹ã…‹")
                        self.speak_with_elevenlabs("ì¹´ë©”ë¼ ëª¨ë“œ ì¼ ë‹¤!")
                    
                    elif cmd_lower == SCREEN_TRIGGER:
                        self.notify("ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ ì¤‘...")
                        self.speak_with_elevenlabs("ìŠ¤í¬ë¦° ìº¡ì²˜í• ê²Œ!")
                        if self.capture_screen():
                            self.screen_mode_waiting = True
                            self.notify("ìº¡ì²˜ ì™„ë£Œ! ì§ˆë¬¸ì„ ì…ë ¥í•´ì¤˜!")
                        else:
                            self.notify("ìº¡ì²˜ ì‹¤íŒ¨!")
                    
                    else:
                        query = self.fix_hangul(cmd)
                        if query:
                            self.notify("ìƒê° ì¤‘... ã…‹ã…‹")
                            if self.screen_mode_waiting:
                                answer = self.get_vision_response(query, TEMP_IMAGE)
                                self.screen_mode_waiting = False
                            else:
                                answer = self.get_ai_response(query)
                            self.notify(answer)
                            self.speak_with_elevenlabs(answer)
                self.full_input = ""
            elif key == keyboard.Key.backspace:
                self.full_input = self.full_input[:-1]
        except Exception as e:
            print(f"âŒ on_press ì—ëŸ¬: {e}")

# --- [ë©”ì¸ ì‹¤í–‰ ë£¨í”„] ---
if __name__ == "__main__":
    print("ğŸš€ í”„ë¡œê·¸ë¨ ì‹œì‘ (Windows 11 ë²„ì „)")
    
    # Music í´ë” ìë™ ìƒì„±
    if not os.path.exists(MUSIC_FOLDER):
        os.makedirs(MUSIC_FOLDER)
        print(f"ğŸ“ {MUSIC_FOLDER} í´ë” ìƒì„±ë¨")
    
    app = QApplication(sys.argv)
    
    cam = SharedCamera()
    print("ğŸ“· SharedCamera ìƒì„±ë¨")
    
    sigs = SignalManager()
    print("ğŸ“¦ SignalManager ìƒì„±ë¨")
    
    orion = OrionBot(sigs, cam)
    print("ğŸ¤– OrionBot ìƒì„±ë¨")
    
    cam_win = CameraWindow(cam, orion.get_gemini_vision)
    print("ğŸ“· CameraWindow ìƒì„±ë¨")
    
    dbg_win = DebugWindow(sigs)
    print("ğŸ–¥ï¸ DebugWindow ìƒì„±ë¨")
    
    sigs.show_camera.connect(cam_win.show)
    sigs.close_camera.connect(cam_win.close_cam)
    sigs.show_debug.connect(dbg_win.show)
    sigs.hide_debug.connect(dbg_win.hide)
    print("ğŸ”— ì‹œê·¸ë„ ì—°ê²° ì™„ë£Œ")
    
    # ì œìŠ¤ì²˜ í•­ìƒ í™œì„±í™”
    orion.gesture_ctrl.start()
    orion.gesture_active = True
    dbg_win.show()
    print("ğŸ‘‹ ì œìŠ¤ì²˜ ì¸ì‹ ìë™ ì‹œì‘ë¨")
    
    listener = keyboard.Listener(on_press=orion.on_press)
    listener.start()
    print("âŒ¨ï¸ í‚¤ë³´ë“œ ë¦¬ìŠ¤ë„ˆ ì‹œì‘ë¨")
    
    print(f"--- [{AI_NAME}] V6.1 Windows 11 ë²„ì „ ê°€ë™ ì¤‘ ---")
    print(f"[TTS] ElevenLabs Voice ID: {ELEVENLABS_VOICE_ID}")
    print("=" * 50)
    print("ğŸ’¡ '123enter' ì…ë ¥ í›„ ì—”í„° â†’ í™œì„±í™”")
    print("ğŸ’¡ 'cameramode' ì…ë ¥ í›„ ì—”í„° â†’ ì¹´ë©”ë¼")
    print("ğŸ’¡ 'gesturemode' ì…ë ¥ í›„ ì—”í„° â†’ ì œìŠ¤ì²˜ í† ê¸€")
    print("ğŸ’¡ 'screenmode' ì…ë ¥ í›„ ì—”í„° â†’ ìŠ¤í¬ë¦° ìº¡ì²˜")
    print("ğŸµ 'playsong [ë…¸ë˜ì´ë¦„]' ì…ë ¥ í›„ ì—”í„° â†’ ìŒì•… ë¬´í•œ ì¬ìƒ")
    print("ğŸ›‘ 'stopsong' ì…ë ¥ í›„ ì—”í„° â†’ ìŒì•… ì¤‘ì§€")
    print("ğŸ”‡ 'ignore' ì…ë ¥ í›„ ì—”í„° â†’ ì±„íŒ… ê°ì§€ ì¼ì‹œì •ì§€ (ìŒì•…ì€ ê³„ì†)")
    print("ğŸ”Š 'ignorex' ì…ë ¥ í›„ ì—”í„° â†’ ì±„íŒ… ê°ì§€ ì¬ê°œ")
    print("ğŸ’¡ '123exit' ì…ë ¥ í›„ ì—”í„° â†’ ì¢…ë£Œ")
    print("ğŸ‘‹ ì œìŠ¤ì²˜ ì¸ì‹: í•­ìƒ í™œì„±í™”ë¨ (ìš°í•˜ë‹¨ ë¯¸ë‹ˆë·°ì–´)")
    print(f"ğŸ“ ìŒì•… í´ë”: ./{MUSIC_FOLDER}/ (ì—¬ê¸°ì— mp3 íŒŒì¼ ë„£ê¸°)")
    print("=" * 50)
    
    sys.exit(app.exec())