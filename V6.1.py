import sys
import subprocess
import os
import datetime
import unicodedata
import base64
import cv2
import threading
import requests
import tempfile
import time
import mediapipe as mp
import pyautogui
import pygame
from anthropic import Anthropic
from tavily import TavilyClient
from pynput import keyboard
from dotenv import load_dotenv
from jamo import jamo_to_hcj
from google import genai
from PyQt6.QtWidgets import QApplication, QMainWindow, QLabel, QVBoxLayout, QPushButton, QWidget, QFrame
from PyQt6.QtCore import QTimer, Qt, pyqtSignal, QObject
from PyQt6.QtGui import QImage, QPixmap
from PIL import Image

# 1. í™˜ê²½ ì„¤ì • ë° API ë¡œë“œ
load_dotenv()
anthropic_client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
tavily = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))
gemini_client = genai.Client(api_key=os.getenv("GOOGLE_API_KEY"))
ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY")

START_TRIGGER = "123enter"
EXIT_TRIGGER = "123exit"
SCREEN_TRIGGER = "screenmode"
CAMERA_TRIGGER = "cameramode"
GESTURE_TRIGGER = "gesturemode"
PLAYSONG_TRIGGER = "playsong"
STOPSONG_TRIGGER = "stopsong"
IGNORE_TRIGGER = "ignore"
IGNOREX_TRIGGER = "ignorex"
AI_NAME = "Orion"
PROFILE_FILE = "user_profile.txt"
TEMP_IMAGE = "temp_capture.png"
CLAUDE_MODEL = "claude-sonnet-4-5-20250929"
MUSIC_FOLDER = "Music"

# ElevenLabs ì„¤ì •
ELEVENLABS_VOICE_ID = "QYrOVogqhHWUzdZFXf0E"
ELEVENLABS_API_URL = f"https://api.elevenlabs.io/v1/text-to-speech/{ELEVENLABS_VOICE_ID}"

# ì“°ë ˆë“œ ê°„ UI í†µì‹ ì„ ìœ„í•œ ì‹ í˜¸ ê´€ë¦¬ì (ë§¥ë¶ GUI ì¶©ëŒ ë°©ì§€)
class SignalManager(QObject):
    show_camera = pyqtSignal()
    close_camera = pyqtSignal()
    show_debug = pyqtSignal()
    hide_debug = pyqtSignal()
    update_debug_frame = pyqtSignal(object)  # í”„ë ˆì„ ì „ë‹¬ìš©

# --- [ìŒì•… í”Œë ˆì´ì–´ í´ë˜ìŠ¤ - pygame ê¸°ë°˜ ì‹¤ì‹œê°„ ë³¼ë¥¨ ì¡°ì ˆ] ---
class MusicPlayer:
    def __init__(self):
        pygame.mixer.init()
        self.is_playing = False
        self.current_song = None
        self.current_filepath = None
        self.normal_volume = 0.2    # ê¸°ë³¸ ë³¼ë¥¨ (0.0 ~ 1.0)
        self.ducked_volume = 0.05   # TTS ì¤‘ ë³¼ë¥¨ (15%)
    
    def duck(self):
        """TTS ì‹œì‘ ì‹œ ìŒì•… ë³¼ë¥¨ ë‚®ì¶”ê¸° (ì‹¤ì‹œê°„, ì¬ì‹œì‘ ì—†ìŒ)"""
        if self.is_playing:
            pygame.mixer.music.set_volume(self.ducked_volume)
            print(f"ğŸ”‰ ìŒì•… ë³¼ë¥¨ ë‚®ì¶¤: {self.normal_volume} â†’ {self.ducked_volume}")
    
    def unduck(self):
        """TTS ëë‚˜ë©´ ìŒì•… ë³¼ë¥¨ ë³µêµ¬ (ì‹¤ì‹œê°„, ì¬ì‹œì‘ ì—†ìŒ)"""
        if self.is_playing:
            pygame.mixer.music.set_volume(self.normal_volume)
            print(f"ğŸ”Š ìŒì•… ë³¼ë¥¨ ë³µêµ¬: {self.ducked_volume} â†’ {self.normal_volume}")
    
    def play(self, song_name):
        """ë…¸ë˜ ì¬ìƒ (ë¬´í•œ ë°˜ë³µ)"""
        self.stop()  # ê¸°ì¡´ ì¬ìƒ ì¤‘ì§€
        
        # íŒŒì¼ëª… ë§¤ì¹­: ê³µë°± â†’ ì–¸ë”ìŠ¤ì½”ì–´, í™•ì¥ì ì—†ìœ¼ë©´ .mp3 ì¶”ê°€
        filename = song_name.strip().replace(" ", "_")
        if not filename.lower().endswith(".mp3"):
            filename += ".mp3"
        
        filepath = os.path.join(MUSIC_FOLDER, filename)
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(filepath):
            # ëŒ€ì†Œë¬¸ì ë¬´ì‹œ ê²€ìƒ‰
            if os.path.exists(MUSIC_FOLDER):
                for f in os.listdir(MUSIC_FOLDER):
                    if f.lower() == filename.lower():
                        filepath = os.path.join(MUSIC_FOLDER, f)
                        break
                else:
                    print(f"âŒ ìŒì•… íŒŒì¼ ì—†ìŒ: {filepath}")
                    return False
            else:
                print(f"âŒ Music í´ë” ì—†ìŒ!")
                return False
        
        try:
            pygame.mixer.music.load(filepath)
            pygame.mixer.music.set_volume(self.normal_volume)
            pygame.mixer.music.play(loops=-1)  # -1 = ë¬´í•œ ë°˜ë³µ
            self.is_playing = True
            self.current_song = song_name
            self.current_filepath = filepath
            print(f"ğŸµ ì¬ìƒ ì‹œì‘: {filename}")
            return True
        except Exception as e:
            print(f"ì¬ìƒ ì—ëŸ¬: {e}")
            return False
    
    def stop(self):
        """ì¬ìƒ ì¤‘ì§€"""
        if self.is_playing:
            pygame.mixer.music.stop()
        self.is_playing = False
        self.current_song = None
        self.current_filepath = None
        print("ğŸ›‘ ìŒì•… ì¤‘ì§€ë¨")

# --- [ê³µìœ  ì¹´ë©”ë¼ ë§¤ë‹ˆì €] ---
class SharedCamera:
    _instance = None
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized: return
        self._initialized = True
        self.cap = cv2.VideoCapture(0)
        self.lock = threading.Lock()
        self.last_frame = None
    
    def read(self):
        with self.lock:
            ret, frame = self.cap.read()
            if ret: self.last_frame = frame.copy()
            return ret, self.last_frame

# --- [ì œìŠ¤ì²˜ ì¸ì‹ ì»¨íŠ¸ë¡¤ëŸ¬] ---
class GestureController:
    def __init__(self, shared_camera, signals):
        self.is_running = False
        self.shared_camera = shared_camera
        self.signals = signals
        self.mp_hands = mp.solutions.hands
        self.mp_draw = mp.solutions.drawing_utils
        self.hands = self.mp_hands.Hands(
            static_image_mode=False, max_num_hands=1,
            model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5
        )
        self.prev_state = None
        self.stable_count = 0
        self.last_time = 0

    def start(self):
        if self.is_running: return
        self.is_running = True
        threading.Thread(target=self._run, daemon=True).start()

    def stop(self):
        self.is_running = False

    def _run(self):
        while self.is_running:
            ret, frame = self.shared_camera.read()
            if not ret or frame is None:
                time.sleep(0.01)
                continue
            
            frame = cv2.flip(frame, 1)
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = self.hands.process(rgb)
            
            debug_frame = frame.copy()
            if results.multi_hand_landmarks:
                for hl in results.multi_hand_landmarks:
                    self.mp_draw.draw_landmarks(debug_frame, hl, self.mp_hands.HAND_CONNECTIONS)
                    self._process_gesture(hl)
            
            # PyQt ì‹ í˜¸ë¥¼ í†µí•´ ì•ˆì „í•˜ê²Œ í”„ë ˆì„ ì „ë‹¬
            self.signals.update_debug_frame.emit(debug_frame)
            time.sleep(0.02)

    def _process_gesture(self, hl):
        tip, pip = hl.landmark[8], hl.landmark[6]
        is_ext = pip.y - tip.y > 0.06
        curr = "UP" if is_ext else "DOWN"
        
        if curr == self.prev_state: self.stable_count += 1
        else:
            self.stable_count = 1
            self.prev_state = curr
            return

        now = time.time()
        if now - self.last_time > 0.6 and self.stable_count >= 3:
            if curr == "UP": pyautogui.scroll(12)
            else: pyautogui.scroll(-12)
            self.last_time = now

# --- [PyQt ê¸°ë°˜ ë””ë²„ê·¸ ìœˆë„ìš° (ì†Œí˜•, ìš°í•˜ë‹¨, í•­ìƒ ìµœìƒë‹¨, í´ë¦­ íˆ¬ê³¼)] ---
class DebugWindow(QMainWindow):
    def __init__(self, signals):
        super().__init__()
        # í´ë¦­ íˆ¬ê³¼ í•µì‹¬: WindowTransparentForInput
        self.setWindowFlags(
            Qt.WindowType.FramelessWindowHint |
            Qt.WindowType.WindowStaysOnTopHint |
            Qt.WindowType.WindowTransparentForInput
        )
        self.setAttribute(Qt.WidgetAttribute.WA_TranslucentBackground)
        
        # ì†Œí˜• ì‚¬ì´ì¦ˆ
        width, height = 120, 90
        screen = QApplication.primaryScreen().geometry()
        self.setGeometry(screen.width() - width - 10, screen.height() - height - 50, width, height)
        
        self.container = QFrame(self)
        self.container.setStyleSheet("background-color: rgba(0, 0, 0, 150); border-radius: 8px;")
        self.container.setFixedSize(width, height)
        
        self.label = QLabel(self.container)
        self.label.setFixedSize(width - 10, height - 10)
        self.label.move(5, 5)
        self.setCentralWidget(self.container)
        signals.update_debug_frame.connect(self.set_image)

    def set_image(self, cv_img):
        rgb = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)
        h, w, ch = rgb.shape
        img = QImage(rgb.data, w, h, ch*w, QImage.Format.Format_RGB888)
        self.label.setPixmap(QPixmap.fromImage(img).scaled(110, 80, Qt.AspectRatioMode.KeepAspectRatio))

# --- [ë¦¬í€´ë“œ ê¸€ë˜ìŠ¤ ìŠ¤íƒ€ì¼ ì¹´ë©”ë¼ ìœ„ì ¯] ---
class CameraWindow(QMainWindow):
    def __init__(self, shared_camera, capture_callback):
        super().__init__()
        self.shared_camera = shared_camera
        self.capture_callback = capture_callback
        
        # ì´ˆì†Œí˜• ìœ„ì ¯ ì„¤ì • (Frameless, Always on Top)
        self.setWindowFlags(Qt.WindowType.FramelessWindowHint | Qt.WindowType.WindowStaysOnTopHint | Qt.WindowType.Tool)
        self.setAttribute(Qt.WidgetAttribute.WA_TranslucentBackground)
        
        # ë””ë²„ê¹…: í™”ë©´ ì¤‘ì•™ ê·¼ì²˜ë¡œ ìœ„ì¹˜ ë³€ê²½
        screen = QApplication.primaryScreen().geometry()
        width, height = 115, 145
        self.setGeometry(100, 100, width, height)  # ë””ë²„ê¹…ìš©: ì¢Œìƒë‹¨ ê·¼ì²˜
        print(f"ğŸ“ í™”ë©´ í¬ê¸°: {screen.width()}x{screen.height()}")
        print(f"ğŸ“ ì°½ ì„¤ì • ìœ„ì¹˜: (100, 100, {width}, {height})")

        # ë¦¬í€´ë“œ ê¸€ë˜ìŠ¤ í”„ë ˆì„
        self.container = QFrame(self)
        self.container.setStyleSheet("""
            QFrame {
                background-color: rgba(15, 15, 22, 180);
                border: 1px solid rgba(255, 255, 255, 30);
                border-radius: 12px;
            }
        """)
        self.container.setFixedSize(width, height)

        self.layout = QVBoxLayout(self.container)
        self.layout.setContentsMargins(6, 6, 6, 6)
        self.layout.setSpacing(5)

        self.image_label = QLabel()
        self.image_label.setStyleSheet("border-radius: 8px; background: #000;")
        self.image_label.setFixedSize(103, 80)
        self.layout.addWidget(self.image_label, alignment=Qt.AlignmentFlag.AlignCenter)

        self.btn_identify = QPushButton("Identify")
        self.btn_identify.setStyleSheet("""
            QPushButton {
                background-color: rgba(60, 130, 250, 170);
                color: white;
                border-radius: 6px;
                font-size: 10px;
                font-weight: bold;
                padding: 4px;
            }
            QPushButton:hover { background-color: rgba(80, 150, 255, 220); }
        """)
        self.btn_identify.clicked.connect(self.take_photo)
        self.layout.addWidget(self.btn_identify)

        self.setCentralWidget(self.container)
        
        # ì¹´ë©”ë¼ ì—°ê²° í™•ì¸
        if not self.shared_camera.cap.isOpened():
            print("âŒ ì¹´ë©”ë¼ ì—´ê¸° ì‹¤íŒ¨! ê¶Œí•œ í™•ì¸ í•„ìš”")
        else:
            print("âœ… ì¹´ë©”ë¼ ì—°ê²° ì„±ê³µ")
        
        self.timer = QTimer()
        self.timer.timeout.connect(self.update_frame)
        self.timer.start(30)

    def show(self):
        """ì˜¤ë²„ë¼ì´ë“œ: ë””ë²„ê¹… + ì°½ í™œì„±í™” ê°•í™”"""
        print("âœ… CameraWindow.show() í˜¸ì¶œë¨")
        super().show()
        self.raise_()  # ì°½ì„ ë§¨ ì•ìœ¼ë¡œ
        self.activateWindow()  # ì°½ í™œì„±í™”
        print(f"ğŸ“ ì‹¤ì œ ì°½ ìœ„ì¹˜: {self.geometry().x()}, {self.geometry().y()}")
        print(f"ğŸ‘ï¸ ì°½ visible ìƒíƒœ: {self.isVisible()}")

    def update_frame(self):
        ret, frame = self.shared_camera.read()
        if ret and frame is not None:
            frame = cv2.flip(frame, 1)
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            h, w, ch = frame.shape
            q_img = QImage(frame.data, w, h, ch * w, QImage.Format.Format_RGB888)
            pixmap = QPixmap.fromImage(q_img).scaled(103, 80, Qt.AspectRatioMode.KeepAspectRatioByExpanding, Qt.TransformationMode.SmoothTransformation)
            self.image_label.setPixmap(pixmap)

    def take_photo(self):
        ret, frame = self.shared_camera.read()
        if ret:
            cv2.imwrite(TEMP_IMAGE, frame)
            threading.Thread(target=self.capture_callback, daemon=True).start()

    def close_cam(self):
        print("ğŸ”´ CameraWindow.close_cam() í˜¸ì¶œë¨")
        self.hide()

# --- [ë©”ì¸ ë´‡ í´ë˜ìŠ¤: ì˜¤ë¦¬ì˜¨ V6.1] ---
class OrionBot:
    def __init__(self, signal_manager, shared_camera):
        self.is_active = False
        self.screen_mode_waiting = False 
        self.ignore_mode = False  # V6.1: ignore ëª¨ë“œ í”Œë˜ê·¸
        self.full_input = ""
        self.short_term_memory = []
        self.signals = signal_manager
        self.shared_camera = shared_camera
        self.gesture_ctrl = GestureController(shared_camera, signal_manager)
        self.gesture_active = False
        self.music_player = MusicPlayer()  # ìŒì•… í”Œë ˆì´ì–´ ì¶”ê°€
        self.load_personal_profile()

    def load_personal_profile(self):
        extra_info = ""
        if os.path.exists(PROFILE_FILE):
            with open(PROFILE_FILE, "r", encoding="utf-8") as f:
                extra_info = f.read()
        
        self.system_prompt = (
            f"ë‹¹ì‹ ì€ ê±´í¬ì˜ ë² í”„ì´ì ì „ìš© AI ë¹„ì„œ '{AI_NAME}'ì´ì•¼! ã…‹ã…‹\n"
            f"[ê±´í¬ ì •ë³´]\n{extra_info}\n"
            "í•µì‹¬ ì§€ì¹¨:\n"
            "1. ë¬´ì¡°ê±´ 'ë°˜ë§'ë¡œ ì¹œêµ¬ì²˜ëŸ¼ ë°ê²Œ ë§í•´ì¤˜!\n"
            "2. ë‹µë³€ì€ ì•Œë¦¼ì°½ìš©ì´ë‹ˆê¹Œ ë¬´ì¡°ê±´ 'í•œ ë¬¸ì¥'ìœ¼ë¡œ ì•„ì£¼ ì§§ê³  í•µì‹¬ë§Œ ë§í•´.\n"
            "3. ì´ë¯¸ì§€ ë¶„ì„ ì‹œì—ëŠ” ì•„ì£¼ êµ¬ì²´ì ì´ê³  ì¬ì¹˜ ìˆê²Œ ì„¤ëª…í•´ì¤˜.\n"
            "4. ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê¸°ì–µí•´ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€ì¤˜."
        )

    def fix_hangul(self, text):
        try:
            combined = jamo_to_hcj(text)
            return unicodedata.normalize('NFC', combined)
        except:
            return unicodedata.normalize('NFC', text)

    def activate_python_app(self):
        """ìµœì†Œí™”ëœ íŒŒì´ì¬ ì•±ì„ í™”ë©´ ë§¨ ì•ìœ¼ë¡œ ê°•ì œ í™œì„±í™”"""
        try:
            script = 'tell application "System Events" to set frontmost of every process whose name contains "Python" to true'
            subprocess.run(["osascript", "-e", script])
            print("ğŸ”„ Python ì•± í™œì„±í™” ì‹œë„")
        except Exception as e:
            print(f"App Activation Error: {e}")

    def capture_screen(self):
        try:
            subprocess.run(["screencapture", "-i", "-x", TEMP_IMAGE], check=True)
            return os.path.exists(TEMP_IMAGE)
        except:
            return False

    def translate_to_english(self, korean_text):
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­"""
        try:
            response = anthropic_client.messages.create(
                model=CLAUDE_MODEL,
                max_tokens=200,
                messages=[{
                    "role": "user", 
                    "content": f"Translate this Korean text to natural English. The name 'ê±´í¬' should be written as 'Gun-hee'. Only output the translation, nothing else:\n\n{korean_text}"
                }]
            )
            result = response.content[0].text.strip()
            
            # í˜¹ì‹œ ëª¨ë¥¼ ì˜ëª»ëœ í‘œê¸° êµì •
            result = result.replace("Geonhee", "Gun-hee")
            result = result.replace("Gunhee", "Gun-hee")
            result = result.replace("Keonhee", "Gun-hee")
            result = result.replace("ê±´í¬", "Gun-hee")
            
            return result
        except Exception as e:
            print(f"Translation Error: {e}")
            return korean_text

    def speak_with_elevenlabs(self, text):
        """ElevenLabs TTSë¡œ ì˜ì–´ ìŒì„± ì¶œë ¥ (ë¹„ë™ê¸°) + ìŒì•… ë³¼ë¥¨ ìë™ ì¡°ì ˆ"""
        def _speak():
            try:
                # ğŸ”‰ TTS ì‹œì‘ ì „ ìŒì•… ë³¼ë¥¨ ë‚®ì¶”ê¸°
                self.music_player.duck()
                
                # í•œêµ­ì–´ â†’ ì˜ì–´ ë²ˆì—­
                english_text = self.translate_to_english(text)
                print(f"[TTS] ë²ˆì—­ë¨: {english_text}")
                
                headers = {
                    "Accept": "audio/mpeg",
                    "Content-Type": "application/json",
                    "xi-api-key": ELEVENLABS_API_KEY
                }
                
                data = {
                    "text": english_text,
                    "model_id": "eleven_turbo_v2_5",
                    "voice_settings": {
                        "stability": 0.5,
                        "similarity_boost": 0.75,
                        "style": 0.3,
                        "use_speaker_boost": True
                    }
                }
                
                response = requests.post(ELEVENLABS_API_URL, json=data, headers=headers)
                
                if response.status_code == 200:
                    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ í›„ ì¬ìƒ
                    with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as f:
                        f.write(response.content)
                        temp_path = f.name
                    
                    # macOSì—ì„œ afplayë¡œ ì¬ìƒ
                    subprocess.run(["afplay", temp_path])
                    os.remove(temp_path)
                else:
                    print(f"[TTS Error] Status: {response.status_code}, {response.text}")
                    
            except Exception as e:
                print(f"[TTS Error] {e}")
            finally:
                # ğŸ”Š TTS ëë‚˜ë©´ ë³¼ë¥¨ ë³µêµ¬
                self.music_player.unduck()
        
        # ë¹„ë™ê¸°ë¡œ ì‹¤í–‰ (ë©”ì¸ ìŠ¤ë ˆë“œ ë¸”ë¡œí‚¹ ë°©ì§€)
        threading.Thread(target=_speak, daemon=True).start()

    def get_vision_response(self, user_text, image_path):
        """ê¸°ì¡´ ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ ë¶„ì„ (Claude)"""
        try:
            with open(image_path, "rb") as f:
                image_data = base64.b64encode(f.read()).decode("utf-8")

            response = anthropic_client.messages.create(
                model=CLAUDE_MODEL,
                max_tokens=300,
                system=self.system_prompt,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "image", "source": {"type": "base64", "media_type": "image/png", "data": image_data}},
                            {"type": "text", "text": user_text}
                        ]
                    }
                ]
            )
            if os.path.exists(image_path): os.remove(image_path)
            return response.content[0].text.strip()
        except Exception as e:
            return f"ì´ë¯¸ì§€ ë¶„ì„í•˜ë‹¤ê°€ ë ‰ ê±¸ë ¸ì–´ ã… ã… : {str(e)}"

    def get_gemini_vision(self):
        """ì‹¤ì‹œê°„ ì¹´ë©”ë¼ ë¶„ì„ (Gemini 2.0 Flash)"""
        try:
            img = Image.open(TEMP_IMAGE)

            response = gemini_client.models.generate_content(
                model="gemini-2.0-flash", 
                contents=[
                    self.system_prompt + "\nì´ ì´ë¯¸ì§€ë¥¼ ë³´ê³  ì¬ì¹˜ ìˆê²Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ë§í•´ì¤˜!",
                    img
                ]
            )
            
            answer = response.text.strip()
            self.notify(answer)
            self.speak_with_elevenlabs(answer)  # TTS ì¶”ê°€
            
            img.close()
            if os.path.exists(TEMP_IMAGE): 
                os.remove(TEMP_IMAGE)
                
        except Exception as e:
            print(f"Gemini Vision Error: {e}")
            self.notify("ëª¨ë¸ì„ ëª» ì°¾ê² ëŒ€! ì´ë¦„ì„ ë‹¤ì‹œ í™•ì¸í•´ë³¼ê²Œ.")

    def get_ai_response(self, user_text):
        """ëª¨ë“  ëŒ€í™”/ê²€ìƒ‰/ì‚¬ê³  ë¡œì§ + ì‹œê°„/ë‚ ì”¨/ë‰´ìŠ¤ ê°•í™”"""
        try:
            user_text = self.fix_hangul(user_text)
            
            now = datetime.datetime.now()
            time_info = f"[í˜„ì¬ ì‹œê°: {now.strftime('%Yë…„ %mì›” %dì¼ %A %Hì‹œ %Më¶„')}]"
            
            force_search_keywords = ["ë‚ ì”¨", "ë‰´ìŠ¤", "ì˜¤ëŠ˜", "ìµœê·¼", "í˜„ì¬", "ì§€ê¸ˆ", "ì‹¤ì‹œê°„", "weather", "news"]
            needs_force_search = any(kw in user_text.lower() for kw in force_search_keywords)
            
            context = ""
            
            if needs_force_search:
                search_prompt = anthropic_client.messages.create(
                    model=CLAUDE_MODEL,
                    max_tokens=50,
                    messages=[{"role": "user", "content": f"'{user_text}'ë¥¼ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ ì˜ì–´ ê²€ìƒ‰ì–´ í•˜ë‚˜ë§Œ ì¶œë ¥í•´. ì˜ˆ: 'Seoul weather today'"}]
                )
                query = search_prompt.content[0].text.strip()
                res = tavily.search(query=query, search_depth="advanced", max_results=3)
                context = "\n\n[ì‹¤ì‹œê°„ ì •ë³´]: " + "\n".join([r['content'] for r in res['results']])
            else:
                thought_res = anthropic_client.messages.create(
                    model=CLAUDE_MODEL,
                    max_tokens=100,
                    messages=[{"role": "user", "content": f"ì§ˆë¬¸: '{user_text}'\nê²€ìƒ‰ í•„ìš”ì‹œ 'SEARCH: [ì˜ì–´ê²€ìƒ‰ì–´]', ë¶ˆí•„ìš”ì‹œ 'NO'ë§Œ ëŒ€ë‹µ."}]
                )
                thought = thought_res.content[0].text.strip()
                
                if "SEARCH:" in thought.upper():
                    query = thought.split(":", 1)[1].strip()
                    res = tavily.search(query=query, search_depth="advanced", max_results=3)
                    context = "\n\n[ì‹¤ì‹œê°„ ì •ë³´]: " + "\n".join([r['content'] for r in res['results']])

            messages = [{"role": m["role"], "content": m["content"]} for m in self.short_term_memory]
            messages.append({"role": "user", "content": f"{time_info}\n{user_text} {context}"})

            response = anthropic_client.messages.create(
                model=CLAUDE_MODEL,
                max_tokens=300,
                system=self.system_prompt,
                messages=messages
            )
            answer = response.content[0].text.strip()
            
            self.short_term_memory.append({"role": "user", "content": user_text})
            self.short_term_memory.append({"role": "assistant", "content": answer})
            if len(self.short_term_memory) > 10: self.short_term_memory.pop(0)
            
            return answer
        except Exception as e:
            return f"ì—”ì§„ ê³¼ë¶€í•˜! ã… ã… : {str(e)}"

    def notify(self, msg):
        subprocess.run(["osascript", "-e", f'display notification "{msg.replace("\"", "'")}" with title "{AI_NAME}"'])

    def on_press(self, key):
        try:
            if hasattr(key, 'char') and key.char:
                self.full_input += key.char
            elif key == keyboard.Key.enter:
                cmd = self.full_input.strip()
                cmd_lower = cmd.lower()
                print(f"ğŸ”¤ ì…ë ¥ëœ ëª…ë ¹: '{cmd}'")  # ë””ë²„ê¹…
                
                if not self.is_active:
                    if cmd.endswith(START_TRIGGER):
                        self.is_active = True
                        print("ğŸŸ¢ ì˜¤ë¦¬ì˜¨ í™œì„±í™”ë¨")
                        self.notify("ì˜¤ë¦¬ì˜¨ V6.1 ì—°ê²° ì™„ë£Œ!")
                        self.speak_with_elevenlabs("ì˜¤ë¦¬ì˜¨ V6.1 ì—°ê²° ì™„ë£Œ!")
                elif self.is_active:
                    # ğŸ”‡ V6.1: ignore ëª¨ë“œì¼ ë•ŒëŠ” ignorexë§Œ ì²˜ë¦¬
                    if self.ignore_mode:
                        if cmd_lower == IGNOREX_TRIGGER:
                            self.ignore_mode = False
                            print("ğŸ”Š Ignore ëª¨ë“œ í•´ì œë¨")
                            self.notify("ì±„íŒ… ê°ì§€ ì¬ê°œ!")
                            self.speak_with_elevenlabs("ë‹¤ì‹œ ë“¤ì„ê²Œ!")
                        # ignore ëª¨ë“œì—ì„œëŠ” ë‹¤ë¥¸ ëª¨ë“  ì…ë ¥ ë¬´ì‹œ
                        self.full_input = ""
                        return
                    
                    # ğŸ”‡ V6.1: ignore ëª¨ë“œ ì§„ì…
                    if cmd_lower == IGNORE_TRIGGER:
                        self.ignore_mode = True
                        print("ğŸ”‡ Ignore ëª¨ë“œ ì§„ì…")
                        self.notify("ì±„íŒ… ê°ì§€ ì¼ì‹œì •ì§€! (ignorexë¡œ í•´ì œ)")
                        self.speak_with_elevenlabs("ì ê¹ ì‰´ê²Œ!")
                    
                    elif cmd.endswith(EXIT_TRIGGER):
                        self.is_active = False
                        self.ignore_mode = False  # ignore ëª¨ë“œë„ í•´ì œ
                        self.gesture_ctrl.stop()
                        self.music_player.stop()  # ìŒì•…ë„ ì¤‘ì§€
                        self.signals.hide_debug.emit()
                        self.signals.close_camera.emit()
                        self.notify("í‡´ê·¼í•œë‹¤! ì´ë”° ë´!")
                        self.speak_with_elevenlabs("í‡´ê·¼í•œë‹¤! ì´ë”° ë´!")
                    
                    # ğŸµ ìŒì•… ì¬ìƒ ëª…ë ¹
                    elif cmd_lower.startswith(PLAYSONG_TRIGGER):
                        song_name = cmd[len(PLAYSONG_TRIGGER):].strip()
                        if song_name:
                            if self.music_player.play(song_name):
                                self.notify(f"ğŸµ {song_name} ì¬ìƒ ì¤‘!")
                                self.speak_with_elevenlabs(f"{song_name} í‹€ì–´ì¤„ê²Œ!")
                            else:
                                self.notify(f"âŒ {song_name} íŒŒì¼ì„ ëª» ì°¾ê² ì–´!")
                                self.speak_with_elevenlabs(f"{song_name} íŒŒì¼ì´ ì—†ì–´!")
                        else:
                            self.notify("ë…¸ë˜ ì´ë¦„ì„ ì…ë ¥í•´ì¤˜!")
                            self.speak_with_elevenlabs("ë…¸ë˜ ì´ë¦„ ì•Œë ¤ì¤˜!")
                    
                    # ğŸ›‘ ìŒì•… ì¤‘ì§€ ëª…ë ¹
                    elif cmd_lower == STOPSONG_TRIGGER:
                        if self.music_player.current_song:
                            song = self.music_player.current_song
                            self.music_player.stop()
                            self.notify(f"ğŸ›‘ {song} ì¤‘ì§€!")
                            self.speak_with_elevenlabs("ìŒì•… ê»ì–´!")
                        else:
                            self.notify("ì¬ìƒ ì¤‘ì¸ ìŒì•…ì´ ì—†ì–´!")
                            self.speak_with_elevenlabs("ì§€ê¸ˆ ì¬ìƒ ì¤‘ì¸ ìŒì•… ì—†ì–´!")
                    
                    elif cmd_lower == GESTURE_TRIGGER:
                        self.gesture_active = not self.gesture_active
                        if self.gesture_active:
                            self.gesture_ctrl.start()
                            self.signals.show_debug.emit()
                            self.notify("ì œìŠ¤ì²˜ ëª¨ë“œ ON!")
                            self.speak_with_elevenlabs("ì œìŠ¤ì²˜ ëª¨ë“œ ON!")
                        else:
                            self.gesture_ctrl.stop()
                            self.signals.hide_debug.emit()
                            self.notify("ì œìŠ¤ì²˜ ëª¨ë“œ OFF!")
                            self.speak_with_elevenlabs("ì œìŠ¤ì²˜ ëª¨ë“œ OFF!")
                    elif cmd_lower == CAMERA_TRIGGER:
                        print("ğŸ¯ ì¹´ë©”ë¼ íŠ¸ë¦¬ê±° ê°ì§€ë¨")
                        self.activate_python_app()
                        print("ğŸ“¡ show_camera ì‹œê·¸ë„ emit ì „")
                        self.signals.show_camera.emit()
                        print("ğŸ“¡ show_camera ì‹œê·¸ë„ emit í›„")
                        self.notify("ì¹´ë©”ë¼ ëª¨ë“œ ì¼ ë‹¤! ã…‹ã…‹")
                        self.speak_with_elevenlabs("ì¹´ë©”ë¼ ëª¨ë“œ ì¼ ë‹¤!")
                    elif cmd_lower == SCREEN_TRIGGER:
                        self.notify("ì˜ì—­ ì„ íƒí•´!")
                        self.speak_with_elevenlabs("ì˜ì—­ ì„ íƒí•´!")
                        if self.capture_screen(): self.screen_mode_waiting = True
                    else:
                        query = self.fix_hangul(cmd)
                        if query:
                            self.notify("ìƒê° ì¤‘... ã…‹ã…‹")
                            if self.screen_mode_waiting:
                                answer = self.get_vision_response(query, TEMP_IMAGE)
                                self.screen_mode_waiting = False
                            else:
                                answer = self.get_ai_response(query)
                            self.notify(answer)
                            self.speak_with_elevenlabs(answer)  # TTS ì¶”ê°€!
                self.full_input = ""
            elif key == keyboard.Key.backspace:
                self.full_input = self.full_input[:-1]
        except Exception as e:
            print(f"âŒ on_press ì—ëŸ¬: {e}")

# --- [ë©”ì¸ ì‹¤í–‰ ë£¨í”„] ---
if __name__ == "__main__":
    print("ğŸš€ í”„ë¡œê·¸ë¨ ì‹œì‘")
    
    # Music í´ë” ìë™ ìƒì„±
    if not os.path.exists(MUSIC_FOLDER):
        os.makedirs(MUSIC_FOLDER)
        print(f"ğŸ“ {MUSIC_FOLDER} í´ë” ìƒì„±ë¨")
    
    app = QApplication(sys.argv)
    
    cam = SharedCamera()
    print("ğŸ“· SharedCamera ìƒì„±ë¨")
    
    sigs = SignalManager()
    print("ğŸ“¦ SignalManager ìƒì„±ë¨")
    
    orion = OrionBot(sigs, cam)
    print("ğŸ¤– OrionBot ìƒì„±ë¨")
    
    cam_win = CameraWindow(cam, orion.get_gemini_vision)
    print("ğŸ“· CameraWindow ìƒì„±ë¨")
    
    dbg_win = DebugWindow(sigs)
    print("ğŸ–¥ï¸ DebugWindow ìƒì„±ë¨")
    
    sigs.show_camera.connect(cam_win.show)
    sigs.close_camera.connect(cam_win.close_cam)
    sigs.show_debug.connect(dbg_win.show)
    sigs.hide_debug.connect(dbg_win.hide)
    print("ğŸ”— ì‹œê·¸ë„ ì—°ê²° ì™„ë£Œ")
    
    # ì œìŠ¤ì²˜ í•­ìƒ í™œì„±í™”
    orion.gesture_ctrl.start()
    orion.gesture_active = True
    dbg_win.show()
    print("ğŸ‘‹ ì œìŠ¤ì²˜ ì¸ì‹ ìë™ ì‹œì‘ë¨")
    
    listener = keyboard.Listener(on_press=orion.on_press)
    listener.start()
    print("âŒ¨ï¸ í‚¤ë³´ë“œ ë¦¬ìŠ¤ë„ˆ ì‹œì‘ë¨")
    
    print(f"--- [{AI_NAME}] V6.1 + Gesture + Music + Ignore í†µí•© ë²„ì „ ê°€ë™ ì¤‘ ---")
    print(f"[TTS] ElevenLabs Voice ID: {ELEVENLABS_VOICE_ID}")
    print("=" * 50)
    print("ğŸ’¡ '123enter' ì…ë ¥ í›„ ì—”í„° â†’ í™œì„±í™”")
    print("ğŸ’¡ 'cameramode' ì…ë ¥ í›„ ì—”í„° â†’ ì¹´ë©”ë¼")
    print("ğŸ’¡ 'gesturemode' ì…ë ¥ í›„ ì—”í„° â†’ ì œìŠ¤ì²˜ í† ê¸€")
    print("ğŸ’¡ 'screenmode' ì…ë ¥ í›„ ì—”í„° â†’ ìŠ¤í¬ë¦° ìº¡ì²˜")
    print("ğŸµ 'playsong [ë…¸ë˜ì´ë¦„]' ì…ë ¥ í›„ ì—”í„° â†’ ìŒì•… ë¬´í•œ ì¬ìƒ")
    print("ğŸ›‘ 'stopsong' ì…ë ¥ í›„ ì—”í„° â†’ ìŒì•… ì¤‘ì§€")
    print("ğŸ”‡ 'ignore' ì…ë ¥ í›„ ì—”í„° â†’ ì±„íŒ… ê°ì§€ ì¼ì‹œì •ì§€ (ìŒì•…ì€ ê³„ì†)")
    print("ğŸ”Š 'ignorex' ì…ë ¥ í›„ ì—”í„° â†’ ì±„íŒ… ê°ì§€ ì¬ê°œ")
    print("ğŸ’¡ '123exit' ì…ë ¥ í›„ ì—”í„° â†’ ì¢…ë£Œ")
    print("ğŸ‘‹ ì œìŠ¤ì²˜ ì¸ì‹: í•­ìƒ í™œì„±í™”ë¨ (ìš°í•˜ë‹¨ ë¯¸ë‹ˆë·°ì–´)")
    print(f"ğŸ“ ìŒì•… í´ë”: ./{MUSIC_FOLDER}/ (ì—¬ê¸°ì— mp3 íŒŒì¼ ë„£ê¸°)")
    print("=" * 50)
    
    sys.exit(app.exec())