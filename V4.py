import sys
import subprocess
import os
import datetime
import unicodedata
import base64
import cv2
import threading
import requests
import tempfile
from anthropic import Anthropic
from tavily import TavilyClient
from pynput import keyboard
from dotenv import load_dotenv
from jamo import jamo_to_hcj
from google import genai
from PyQt6.QtWidgets import QApplication, QMainWindow, QLabel, QVBoxLayout, QPushButton, QWidget, QFrame
from PyQt6.QtCore import QTimer, Qt, pyqtSignal, QObject
from PyQt6.QtGui import QImage, QPixmap
from PIL import Image

# 1. í™˜ê²½ ì„¤ì • ë° API ë¡œë“œ
load_dotenv()
anthropic_client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
tavily = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))
gemini_client = genai.Client(api_key=os.getenv("GOOGLE_API_KEY"))
ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY")

START_TRIGGER = "123enter"
EXIT_TRIGGER = "123exit"
SCREEN_TRIGGER = "screenmode"
CAMERA_TRIGGER = "cameramode"
AI_NAME = "Orion"
PROFILE_FILE = "user_profile.txt"
TEMP_IMAGE = "temp_capture.png"
CLAUDE_MODEL = "claude-sonnet-4-5-20250929"

# ElevenLabs ì„¤ì •
ELEVENLABS_VOICE_ID = "QYrOVogqhHWUzdZFXf0E"
ELEVENLABS_API_URL = f"https://api.elevenlabs.io/v1/text-to-speech/{ELEVENLABS_VOICE_ID}"

# ì“°ë ˆë“œ ê°„ UI í†µì‹ ì„ ìœ„í•œ ì‹ í˜¸ ê´€ë¦¬ì (ë§¥ë¶ GUI ì¶©ëŒ ë°©ì§€)
class SignalManager(QObject):
    show_camera = pyqtSignal()
    close_camera = pyqtSignal()

# --- [ë¦¬í€´ë“œ ê¸€ë˜ìŠ¤ ìŠ¤íƒ€ì¼ ì¹´ë©”ë¼ ìœ„ì ¯] ---
class CameraWindow(QMainWindow):
    def __init__(self, capture_callback):
        super().__init__()
        self.capture_callback = capture_callback
        
        # ì´ˆì†Œí˜• ìœ„ì ¯ ì„¤ì • (Frameless, Always on Top)
        self.setWindowFlags(Qt.WindowType.FramelessWindowHint | Qt.WindowType.WindowStaysOnTopHint | Qt.WindowType.Tool)
        self.setAttribute(Qt.WidgetAttribute.WA_TranslucentBackground)
        
        # ë””ë²„ê¹…: í™”ë©´ ì¤‘ì•™ ê·¼ì²˜ë¡œ ìœ„ì¹˜ ë³€ê²½
        screen = QApplication.primaryScreen().geometry()
        width, height = 115, 145
        self.setGeometry(screen.widthx) - width - 5, screen.height() - height - 45, width, height)
        # self.setGeometry(100, 100, width, height)  # ë””ë²„ê¹…ìš©: ì¢Œìƒë‹¨ ê·¼ì²˜
        print(f"ğŸ“ í™”ë©´ í¬ê¸°: {screen.width()}x{screen.height()}")
        print(f"ğŸ“ ì°½ ì„¤ì • ìœ„ì¹˜: (100, 100, {width}, {height})")

        # ë¦¬í€´ë“œ ê¸€ë˜ìŠ¤ í”„ë ˆì„
        self.container = QFrame(self)
        self.container.setStyleSheet("""
            QFrame {
                background-color: rgba(15, 15, 22, 180);
                border: 1px solid rgba(255, 255, 255, 30);
                border-radius: 12px;
            }
        """)
        self.container.setFixedSize(width, height)

        self.layout = QVBoxLayout(self.container)
        self.layout.setContentsMargins(6, 6, 6, 6)
        self.layout.setSpacing(5)

        self.image_label = QLabel()
        self.image_label.setStyleSheet("border-radius: 8px; background: #000;")
        self.image_label.setFixedSize(103, 80)
        self.layout.addWidget(self.image_label, alignment=Qt.AlignmentFlag.AlignCenter)

        self.btn_identify = QPushButton("Identify")
        self.btn_identify.setStyleSheet("""
            QPushButton {
                background-color: rgba(60, 130, 250, 170);
                color: white;
                border-radius: 6px;
                font-size: 10px;
                font-weight: bold;
                padding: 4px;
            }
            QPushButton:hover { background-color: rgba(80, 150, 255, 220); }
        """)
        self.btn_identify.clicked.connect(self.take_photo)
        self.layout.addWidget(self.btn_identify)

        self.setCentralWidget(self.container)
        
        # ì¹´ë©”ë¼ ì´ˆê¸°í™” + ë””ë²„ê¹…
        self.cap = cv2.VideoCapture(0)
        if not self.cap.isOpened():
            print("âŒ ì¹´ë©”ë¼ ì—´ê¸° ì‹¤íŒ¨! ê¶Œí•œ í™•ì¸ í•„ìš”")
        else:
            print("âœ… ì¹´ë©”ë¼ ì—°ê²° ì„±ê³µ")
        
        self.timer = QTimer()
        self.timer.timeout.connect(self.update_frame)
        self.timer.start(30)

    def show(self):
        """ì˜¤ë²„ë¼ì´ë“œ: ë””ë²„ê¹… + ì°½ í™œì„±í™” ê°•í™”"""
        print("âœ… CameraWindow.show() í˜¸ì¶œë¨")
        super().show()
        self.raise_()  # ì°½ì„ ë§¨ ì•ìœ¼ë¡œ
        self.activateWindow()  # ì°½ í™œì„±í™”
        print(f"ğŸ“ ì‹¤ì œ ì°½ ìœ„ì¹˜: {self.geometry().x()}, {self.geometry().y()}")
        print(f"ğŸ‘ï¸ ì°½ visible ìƒíƒœ: {self.isVisible()}")

    def update_frame(self):
        ret, frame = self.cap.read()
        if ret:
            frame = cv2.flip(frame, 1)
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            h, w, ch = frame.shape
            q_img = QImage(frame.data, w, h, ch * w, QImage.Format.Format_RGB888)
            pixmap = QPixmap.fromImage(q_img).scaled(103, 80, Qt.AspectRatioMode.KeepAspectRatioByExpanding, Qt.TransformationMode.SmoothTransformation)
            self.image_label.setPixmap(pixmap)

    def take_photo(self):
        ret, frame = self.cap.read()
        if ret:
            cv2.imwrite(TEMP_IMAGE, frame)
            threading.Thread(target=self.capture_callback, daemon=True).start()

    def close_cam(self):
        print("ğŸ”´ CameraWindow.close_cam() í˜¸ì¶œë¨")
        self.hide()

# --- [ë©”ì¸ ë´‡ í´ë˜ìŠ¤: ì˜¤ë¦¬ì˜¨ V4] ---
class OrionBot:
    def __init__(self, signal_manager):
        self.is_active = False
        self.screen_mode_waiting = False 
        self.full_input = ""
        self.short_term_memory = []
        self.signals = signal_manager
        self.load_personal_profile()

    def load_personal_profile(self):
        extra_info = ""
        if os.path.exists(PROFILE_FILE):
            with open(PROFILE_FILE, "r", encoding="utf-8") as f:
                extra_info = f.read()
        
        self.system_prompt = (
            f"ë‹¹ì‹ ì€ ê±´í¬ì˜ ë² í”„ì´ì ì „ìš© AI ë¹„ì„œ '{AI_NAME}'ì´ì•¼! ã…‹ã…‹\n"
            f"[ê±´í¬ ì •ë³´]\n{extra_info}\n"
            "í•µì‹¬ ì§€ì¹¨:\n"
            "1. ë¬´ì¡°ê±´ 'ë°˜ë§'ë¡œ ì¹œêµ¬ì²˜ëŸ¼ ë°ê²Œ ë§í•´ì¤˜!\n"
            "2. ë‹µë³€ì€ ì•Œë¦¼ì°½ìš©ì´ë‹ˆê¹Œ ë¬´ì¡°ê±´ 'í•œ ë¬¸ì¥'ìœ¼ë¡œ ì•„ì£¼ ì§§ê³  í•µì‹¬ë§Œ ë§í•´.\n"
            "3. ì´ë¯¸ì§€ ë¶„ì„ ì‹œì—ëŠ” ì•„ì£¼ êµ¬ì²´ì ì´ê³  ì¬ì¹˜ ìˆê²Œ ì„¤ëª…í•´ì¤˜.\n"
            "4. ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê¸°ì–µí•´ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€ì¤˜."
        )

    def fix_hangul(self, text):
        try:
            combined = jamo_to_hcj(text)
            return unicodedata.normalize('NFC', combined)
        except:
            return unicodedata.normalize('NFC', text)

    def activate_python_app(self):
        """ìµœì†Œí™”ëœ íŒŒì´ì¬ ì•±ì„ í™”ë©´ ë§¨ ì•ìœ¼ë¡œ ê°•ì œ í™œì„±í™”"""
        try:
            script = 'tell application "System Events" to set frontmost of every process whose name contains "Python" to true'
            subprocess.run(["osascript", "-e", script])
            print("ğŸ”„ Python ì•± í™œì„±í™” ì‹œë„")
        except Exception as e:
            print(f"App Activation Error: {e}")

    def capture_screen(self):
        try:
            subprocess.run(["screencapture", "-i", "-x", TEMP_IMAGE], check=True)
            return os.path.exists(TEMP_IMAGE)
        except:
            return False

    def translate_to_english(self, korean_text):
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­"""
        try:
            response = anthropic_client.messages.create(
                model=CLAUDE_MODEL,
                max_tokens=200,
                messages=[{
                    "role": "user", 
                    "content": f"Translate this Korean text to natural English. The name 'ê±´í¬' should be written as 'Gun-hee'. Only output the translation, nothing else:\n\n{korean_text}"
                }]
            )
            result = response.content[0].text.strip()
            
            # í˜¹ì‹œ ëª¨ë¥¼ ì˜ëª»ëœ í‘œê¸° êµì •
            result = result.replace("Geonhee", "Gun-hee")
            result = result.replace("Gunhee", "Gun-hee")
            result = result.replace("Keonhee", "Gun-hee")
            result = result.replace("ê±´í¬", "Gun-hee")
            
            return result
        except Exception as e:
            print(f"Translation Error: {e}")
            return korean_text

    def speak_with_elevenlabs(self, text):
        """ElevenLabs TTSë¡œ ì˜ì–´ ìŒì„± ì¶œë ¥ (ë¹„ë™ê¸°)"""
        def _speak():
            try:
                # í•œêµ­ì–´ â†’ ì˜ì–´ ë²ˆì—­
                english_text = self.translate_to_english(text)
                print(f"[TTS] ë²ˆì—­ë¨: {english_text}")
                
                headers = {
                    "Accept": "audio/mpeg",
                    "Content-Type": "application/json",
                    "xi-api-key": ELEVENLABS_API_KEY
                }
                
                data = {
                    "text": english_text,
                    "model_id": "eleven_turbo_v2_5",
                    "voice_settings": {
                        "stability": 0.5,
                        "similarity_boost": 0.75,
                        "style": 0.3,
                        "use_speaker_boost": True
                    }
                }
                
                response = requests.post(ELEVENLABS_API_URL, json=data, headers=headers)
                
                if response.status_code == 200:
                    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ í›„ ì¬ìƒ
                    with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as f:
                        f.write(response.content)
                        temp_path = f.name
                    
                    # macOSì—ì„œ afplayë¡œ ì¬ìƒ
                    subprocess.run(["afplay", temp_path])
                    os.remove(temp_path)
                else:
                    print(f"[TTS Error] Status: {response.status_code}, {response.text}")
                    
            except Exception as e:
                print(f"[TTS Error] {e}")
        
        # ë¹„ë™ê¸°ë¡œ ì‹¤í–‰ (ë©”ì¸ ìŠ¤ë ˆë“œ ë¸”ë¡œí‚¹ ë°©ì§€)
        threading.Thread(target=_speak, daemon=True).start()

    def get_vision_response(self, user_text, image_path):
        """ê¸°ì¡´ ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ ë¶„ì„ (Claude)"""
        try:
            with open(image_path, "rb") as f:
                image_data = base64.b64encode(f.read()).decode("utf-8")

            response = anthropic_client.messages.create(
                model=CLAUDE_MODEL,
                max_tokens=300,
                system=self.system_prompt,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "image", "source": {"type": "base64", "media_type": "image/png", "data": image_data}},
                            {"type": "text", "text": user_text}
                        ]
                    }
                ]
            )
            if os.path.exists(image_path): os.remove(image_path)
            return response.content[0].text.strip()
        except Exception as e:
            return f"ì´ë¯¸ì§€ ë¶„ì„í•˜ë‹¤ê°€ ë ‰ ê±¸ë ¸ì–´ ã… ã… : {str(e)}"

    def get_gemini_vision(self):
        """ì‹¤ì‹œê°„ ì¹´ë©”ë¼ ë¶„ì„ (Gemini 2.0 Flash)"""
        try:
            img = Image.open(TEMP_IMAGE)

            response = gemini_client.models.generate_content(
                model="gemini-2.0-flash", 
                contents=[
                    self.system_prompt + "\nì´ ì´ë¯¸ì§€ë¥¼ ë³´ê³  ì¬ì¹˜ ìˆê²Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ë§í•´ì¤˜!",
                    img
                ]
            )
            
            answer = response.text.strip()
            self.notify(answer)
            self.speak_with_elevenlabs(answer)  # TTS ì¶”ê°€
            
            img.close()
            if os.path.exists(TEMP_IMAGE): 
                os.remove(TEMP_IMAGE)
                
        except Exception as e:
            print(f"Gemini Vision Error: {e}")
            self.notify("ëª¨ë¸ì„ ëª» ì°¾ê² ëŒ€! ì´ë¦„ì„ ë‹¤ì‹œ í™•ì¸í•´ë³¼ê²Œ.")

    def get_ai_response(self, user_text):
        """V2ì˜ ëª¨ë“  ëŒ€í™”/ê²€ìƒ‰/ì‚¬ê³  ë¡œì§ ë³µêµ¬ + ì‹œê°„/ë‚ ì”¨/ë‰´ìŠ¤ ê°•í™”"""
        try:
            user_text = self.fix_hangul(user_text)
            
            now = datetime.datetime.now()
            time_info = f"[í˜„ì¬ ì‹œê°: {now.strftime('%Yë…„ %mì›” %dì¼ %A %Hì‹œ %Më¶„')}]"
            
            force_search_keywords = ["ë‚ ì”¨", "ë‰´ìŠ¤", "ì˜¤ëŠ˜", "ìµœê·¼", "í˜„ì¬", "ì§€ê¸ˆ", "ì‹¤ì‹œê°„", "weather", "news"]
            needs_force_search = any(kw in user_text.lower() for kw in force_search_keywords)
            
            context = ""
            
            if needs_force_search:
                search_prompt = anthropic_client.messages.create(
                    model=CLAUDE_MODEL,
                    max_tokens=50,
                    messages=[{"role": "user", "content": f"'{user_text}'ë¥¼ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ ì˜ì–´ ê²€ìƒ‰ì–´ í•˜ë‚˜ë§Œ ì¶œë ¥í•´. ì˜ˆ: 'Seoul weather today'"}]
                )
                query = search_prompt.content[0].text.strip()
                res = tavily.search(query=query, search_depth="advanced", max_results=3)
                context = "\n\n[ì‹¤ì‹œê°„ ì •ë³´]: " + "\n".join([r['content'] for r in res['results']])
            else:
                thought_res = anthropic_client.messages.create(
                    model=CLAUDE_MODEL,
                    max_tokens=100,
                    messages=[{"role": "user", "content": f"ì§ˆë¬¸: '{user_text}'\nê²€ìƒ‰ í•„ìš”ì‹œ 'SEARCH: [ì˜ì–´ê²€ìƒ‰ì–´]', ë¶ˆí•„ìš”ì‹œ 'NO'ë§Œ ëŒ€ë‹µ."}]
                )
                thought = thought_res.content[0].text.strip()
                
                if "SEARCH:" in thought.upper():
                    query = thought.split(":", 1)[1].strip()
                    res = tavily.search(query=query, search_depth="advanced", max_results=3)
                    context = "\n\n[ì‹¤ì‹œê°„ ì •ë³´]: " + "\n".join([r['content'] for r in res['results']])

            messages = [{"role": m["role"], "content": m["content"]} for m in self.short_term_memory]
            messages.append({"role": "user", "content": f"{time_info}\n{user_text} {context}"})

            response = anthropic_client.messages.create(
                model=CLAUDE_MODEL,
                max_tokens=300,
                system=self.system_prompt,
                messages=messages
            )
            answer = response.content[0].text.strip()
            
            self.short_term_memory.append({"role": "user", "content": user_text})
            self.short_term_memory.append({"role": "assistant", "content": answer})
            if len(self.short_term_memory) > 10: self.short_term_memory.pop(0)
            
            return answer
        except Exception as e:
            return f"ì—”ì§„ ê³¼ë¶€í•˜! ã… ã… : {str(e)}"

    def notify(self, msg):
        subprocess.run(["osascript", "-e", f'display notification "{msg.replace("\"", "'")}" with title "{AI_NAME}"'])

    def on_press(self, key):
        try:
            if hasattr(key, 'char') and key.char:
                self.full_input += key.char
            elif key == keyboard.Key.enter:
                cmd = self.full_input.strip()
                print(f"ğŸ”¤ ì…ë ¥ëœ ëª…ë ¹: '{cmd}'")  # ë””ë²„ê¹…
                
                if not self.is_active:
                    if cmd.endswith(START_TRIGGER):
                        self.is_active = True
                        print("ğŸŸ¢ ì˜¤ë¦¬ì˜¨ í™œì„±í™”ë¨")
                        self.notify("ì˜¤ë¦¬ì˜¨ V4 ì—°ê²° ì™„ë£Œ!")
                        self.speak_with_elevenlabs("ì˜¤ë¦¬ì˜¨ V4 ì—°ê²° ì™„ë£Œ!")
                elif self.is_active:
                    if cmd.endswith(EXIT_TRIGGER):
                        self.is_active = False
                        self.signals.close_camera.emit()
                        self.notify("í‡´ê·¼í•œë‹¤! ì´ë”° ë´!")
                        self.speak_with_elevenlabs("í‡´ê·¼í•œë‹¤! ì´ë”° ë´!")
                    elif cmd == CAMERA_TRIGGER:
                        print("ğŸ¯ ì¹´ë©”ë¼ íŠ¸ë¦¬ê±° ê°ì§€ë¨")
                        self.activate_python_app()
                        print("ğŸ“¡ show_camera ì‹œê·¸ë„ emit ì „")
                        self.signals.show_camera.emit()
                        print("ğŸ“¡ show_camera ì‹œê·¸ë„ emit í›„")
                        self.notify("ì¹´ë©”ë¼ ëª¨ë“œ ì¼ ë‹¤! ã…‹ã…‹")
                        self.speak_with_elevenlabs("ì¹´ë©”ë¼ ëª¨ë“œ ì¼ ë‹¤!")
                    elif cmd == SCREEN_TRIGGER:
                        self.notify("ì˜ì—­ ì„ íƒí•´!")
                        self.speak_with_elevenlabs("ì˜ì—­ ì„ íƒí•´!")
                        if self.capture_screen(): self.screen_mode_waiting = True
                    else:
                        query = self.fix_hangul(cmd)
                        if query:
                            self.notify("ìƒê° ì¤‘... ã…‹ã…‹")
                            if self.screen_mode_waiting:
                                answer = self.get_vision_response(query, TEMP_IMAGE)
                                self.screen_mode_waiting = False
                            else:
                                answer = self.get_ai_response(query)
                            self.notify(answer)
                            self.speak_with_elevenlabs(answer)  # TTS ì¶”ê°€!
                self.full_input = ""
            elif key == keyboard.Key.backspace:
                self.full_input = self.full_input[:-1]
        except Exception as e:
            print(f"âŒ on_press ì—ëŸ¬: {e}")

# --- [ë©”ì¸ ì‹¤í–‰ ë£¨í”„] ---
if __name__ == "__main__":
    print("ğŸš€ í”„ë¡œê·¸ë¨ ì‹œì‘")
    app = QApplication(sys.argv)
    
    sigs = SignalManager()
    print("ğŸ“¦ SignalManager ìƒì„±ë¨")
    
    cam_win = CameraWindow(capture_callback=None)
    print("ğŸ“· CameraWindow ìƒì„±ë¨")
    
    orion = OrionBot(sigs)
    print("ğŸ¤– OrionBot ìƒì„±ë¨")
    
    cam_win.capture_callback = orion.get_gemini_vision
    
    sigs.show_camera.connect(cam_win.show)
    sigs.close_camera.connect(cam_win.close_cam)
    print("ğŸ”— ì‹œê·¸ë„ ì—°ê²° ì™„ë£Œ")
    
    listener = keyboard.Listener(on_press=orion.on_press)
    listener.start()
    print("âŒ¨ï¸ í‚¤ë³´ë“œ ë¦¬ìŠ¤ë„ˆ ì‹œì‘ë¨")
    
    print(f"--- [{AI_NAME}] V4 ë””ë²„ê·¸ ë²„ì „ ê°€ë™ ì¤‘ ---")
    print(f"[TTS] ElevenLabs Voice ID: {ELEVENLABS_VOICE_ID}")
    print("=" * 50)
    print("ğŸ’¡ '123enter' ì…ë ¥ í›„ ì—”í„° â†’ í™œì„±í™”")
    print("ğŸ’¡ 'cameramode' ì…ë ¥ í›„ ì—”í„° â†’ ì¹´ë©”ë¼")
    print("=" * 50)
    
    sys.exit(app.exec())